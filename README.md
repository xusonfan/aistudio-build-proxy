# AI API 代理服务器 (Go)

这是一个用 Go 语言编写的高性能代理服务器，专为大型语言模型（LLM）API 设计。它作为一个中间层，接收来自各种客户端的请求，通过 WebSocket 隧道将其安全地转发给一个或多个浏览器客户端（例如浏览器扩展）来执行，并将结果返回给原始请求方。

该项目的主要亮点是其内置的协议转换能力，能够将符合 OpenAI `chat/completions` API 格式的请求动态转换为 Google Gemini API 格式，从而允许为 OpenAI 设计的应用程序无缝地使用 Gemini 模型。

## ✨ 核心功能

- **OpenAI 到 Gemini 的协议转换**：自动将 OpenAI 格式的 API 请求（包括流式和非流式）转换为 Gemini 格式，并相应地转换响应。
- **WebSocket 隧道**：通过 WebSocket 与后端客户端（如浏览器扩展）建立安全、持久的通信隧道，用于转发和接收 API 请求。
- **连接池与负载均衡**：为每个用户维护一个 WebSocket 连接池。当用户有多个客户端连接时，服务器采用轮询（Round-Robin）策略来分发请求，实现负载均衡和高可用性。
- **连接失败重试**：当一个 WebSocket 客户端发送请求失败或超时，服务器会自动在用户的连接池中选择下一个可用的客户端进行重试，增强了请求的成功率。
- **原生 Gemini 代理**：除了协议转换，还支持直接代理对 Gemini API (`v1` 和 `v1beta`) 的原生请求。
- **兼容 OpenAI 模型列表**：提供 `/v1/models` 端点，可将 Google Gemini 的模型列表转换为 OpenAI 格式，增强了兼容性。
- **API 使用统计**：记录每个模型每日的成功调用次数，以及按 HTTP 状态码分类的错误请求次数，并将数据持久化到 `usage_stats.json`。
- **可视化仪表盘**：通过内置的 Web 服务器提供一个状态页面 (`status.html`)，使用 ECharts 动态展示 API 调用统计数据。
- **CORS 支持**：内置 CORS 中间件，允许来自任何来源的跨域请求。

## 🚀 工作原理

1.  **浏览器客户端连接**：一个或多个后端客户端（例如，一个定制的浏览器扩展）通过 WebSocket 连接到代理服务器。每个客户端使用一个唯一的 `auth_token` 作为其用户标识（`UserID`），以便服务器对它们进行分组管理。
2.  **API 请求入口**：第三方应用程序（例如，一个桌面客户端）向代理服务器发送 HTTP API 请求。
    - 对于 **OpenAI 兼容请求**，应用程序需在请求头中提供 Google Gemini 的 API 密钥（例如 `Authorization: Bearer <YOUR_GEMINI_KEY>`）。
    - 对于 **原生 Gemini 请求**，代理服务器不强制要求密钥，期望浏览器客户端自行处理身份验证。
3.  **请求路由与密钥传递**：
    - 服务器从 HTTP 请求中提取出 API 密钥，并将其包含在即将转发的信息中。
    - 服务器根据 WebSocket 连接时提供的 `auth_token` 识别出对应的用户（`UserID`），并从该用户的连接池中通过轮询策略选择一个可用的浏览器客户端。
4.  **WebSocket 转发**：服务器将原始 HTTP 请求（包括方法、URL、请求体）以及处理后的头部（包含 API 密钥）封装成一个 JSON 消息，通过选定的 WebSocket 连接安全地发送给浏览器客户端。对于 OpenAI 请求，请求体会被预先转换为 Gemini 格式。
5.  **浏览器执行**：浏览器客户端收到消息后，使用其自身的网络能力（例如 `fetch` API）向真正的 Google Gemini API 发送请求。这样做的好处是，请求是从用户的浏览器直接发出的，可以利用用户的登录状态和网络环境。
6.  **结果返回**：浏览器客户端获取到 Gemini API 的响应后，将其封装成 JSON 消息，通过 WebSocket 发回给代理服务器。对于流式响应，数据会以 `stream_chunk` 的形式分块返回。
7.  **响应中继**：代理服务器接收到来自浏览器的响应，将其解析并转换回原始请求方所期望的格式（例如，将 Gemini 的响应转换为 OpenAI 格式），然后通过 HTTP 连接返回给第三方应用程序。

## 🛡️ 健壮性与错误处理

服务器在设计上考虑了多种故障场景，以确保其稳定运行：

- **心跳检测**：WebSocket 连接实现了心跳机制。服务器会定期检查连接的活动状态，并自动断开和清理长时间未响应的“僵尸”连接，防止资源泄露。
- **请求超时**：所有通过 WebSocket 转发的请求都设置了超时时间。如果后端客户端在规定时间内未能响应，请求将被取消，并向原始调用方返回 `504 Gateway Timeout` 错误。
- **连接重试**：如果一个用户的某个 WebSocket 客户端连接失败（例如，发送消息时出错或响应超时），代理服务器会自动尝试该用户的下一个可用连接，直到所有连接都尝试完毕。
- **错误透传**：当浏览器客户端在执行 API 请求时遇到错误（例如，Gemini API 返回了非 200 的状态码），该错误信息会被封装并传回给代理服务器，最终以合适的 HTTP 状态码（如 `400 Bad Request` 或 `502 Bad Gateway`）返回给原始请求方。
- **原子化统计数据写入**：为了防止在写入统计数据时因程序中断而导致文件损坏，服务器采用“写入临时文件后重命名”的原子操作来确保数据文件的完整性。

## ️ API 端点

- `ws://<host>:5345/v1/ws?auth_token=<token>`：WebSocket 连接端点，供后端客户端使用。
- `http://<host>:5345/v1/chat/completions`：兼容 OpenAI 的聊天补全接口。
- `http://<host>:5345/v1/models`：兼容 OpenAI 的模型列表接口。
- `http://<host>:5345/v1beta/*` 和 `http://<host>:5345/v1/*`：原生 Gemini API 的代理端点。
- `http://<host>:5345/`：显示 API 使用统计的仪表盘页面。
- `http://<host>:5345/stats`：以 JSON 格式提供原始统计数据。

## ⚙️ 如何运行

1.  **构建**：
    ```bash
    go build -o proxy.exe main.go
    ```
2.  **运行**：
    ```bash
    ./proxy.exe
    ```
    服务器将在默认端口 `5345` 上启动。

## 📊 统计与监控

项目会在本地生成一个 `usage_stats.json` 文件，用于存储所有 API 的调用记录。你可以通过访问服务器的根 URL（例如 `http://localhost:5345`）来查看一个可视化的图表，它展示了不同模型随时间变化的调用情况。